\section{Rainfall-Runoff modelling}
In this section we give some brief explanations of examples of two different types 
of classical rainfall-runoff models. These two types are
\begin{enumerate}
    \item Conceptual models: These models can often be referred to as so called "grey box" models \citationneeded as many important physical effects are replaced by parametrized models.
    \item Process-based models: These models are more physics-based than the conceptual models. 
\end{enumerate}
Because they attempt to predict runoff using physically modelled processes process based models
have been referred to as the models of choice when predicting runoff in ungauged 
 (meaning basins where there is no forcing data available for calibration) basins. 
 \cite{lstm_third_paper} argues that this is not necessarily the case and data driven 
 machine learning models can also give state of the art predictions.
 While this may be the case, we still argue that the ideal scenario would be having 
 a well performing process-based model. This is because all models are likely to 
 have a selection of outlier basins where they perform exceptionally poorly. A model 
 incorporating physics is in this case likely to be easier interpretable, leading 
 to the possibility of being able to know whether one can trust a specific prediction or not. 
 For ungauged basins this is a clear benefit.
\subsection{Sacramento Soil Moisture Accounting}
The Sacramento Soil Moisture Accounting (SAC-SMA) \cite{SAC-SMA} rainfall-runoff model is a widely 
used hydrological and serves the purpose of being a conceptual model in this thesis.
Originally introduced in 1973 the model has since been modified and improved by 
several other papers \cite{SAC-SMA-physics}.
\begin{align}
    \bm{S}^1(t + \Delta t,x) &= \bm{S}(t,x) + f\left(\bm{S}(t,x),P(t+\Delta t,x),E_p (t+\Delta t, x), \bm{V}(x) \right) \\
    \bm{S}^{(n+1)}(t+\Delta t,x) &= \bm{S}^{(n)}(t+\Delta t, x) \nonumber \\
    &+ f\left( \bm{S}^{(n)}(t+\Delta t, x), \bm{q}^{(n+1)}(t+\Delta t,x),P_x(t+\Delta t,x),\bm{V}(x)  \right)
\end{align}

\subsection{The Variable Infiltration Capacity model}
The Variable Infiltration Capacity (VIC) model \cite{VIC} is a process driven model 
with emphasis on simulating the physical process of water running through soil.
The model was created to be used in a GCM (General Circulation Model), but it can 
also be used for rainfall-runoff modelling. The rainfall-runoff modelling aspect 
of the model is presented in short here for context. It works by splitting the catchment 
area into $N$ land coverage categories and the ground (vertically) into several soil layers. 
One upper soil layer and one lower soil layer. This is an attempt at physically 
representing underground streamflow\footnote{While \cite{VIC} originally used a two layer setup, later versions of the VIC model have introduced more layers. When introducing the (still used) frozen soil modelling aspect to VIC \cite{VICFrozenSoil}.}.

The runoff modelling is split into three parts:
\begin{enumerate}
	\item Surface runoff from bare soil
	\item Subsurface runoff from bare soil
	\item Covered surfaces (Types of land cover)
	\item Snow and frozen soil representations.
\end{enumerate}
Surface runoff is a simpler task to model than subsurface runoff. In VIC the physical 
modelling is based on energy equations. We begin by describing the maximum soil 
moisture content of layer 1.
\begin{equation}
W^c_1 = \frac{i_m}{1+b_i} \label{VIC max moisture surface}
\end{equation}

Using (\ref{VIC max moisture surface}) we can get an expression for the surface 
runoff\footnote{For a complete derivation of this we refer to \cite{VIC}.} written 
as 
\begin{equation}
Q_d\left[ N+1 \right] \cdot \Delta t = 
\begin{cases}
	P \cdot \Delta t - W_1^c + W_i^- \left[ N+1 \right], &  i_0 + P \Delta t \geq i_m\\
	P \cdot \Delta t - W_1^c + W_1^- \left[ N+1 \right] \\ + W_1^c \left[  1 - 
    \frac{i_0 + P \cdot \Delta t}{i_m} \right]^{l+b_i}, & i + P \cdot \Delta t \leq i_m
\end{cases}
\end{equation}
Information about the soil moisture of the previous time step is stores in the 
relationship between previous upper layer soil moisture $W_1^-$ and the new 
upper layer soil moisture 
\begin{equation}
	W_1^+ \left[N+1 \right] = W_1^- \left[ N+1  \right] + \left( P - Q_d\left[ 
N+1 \right] - Q_{12}\left[ N+1 \right] - E_1 \right) \cdot \Delta t
\end{equation}
Based on this new soil layer 1 moisture we then get an expression for the flow 
from soil layer 1 to soil layer 2:
\begin{equation}
	Q_{12}\left[ N + 1 \right] = K_s \left( \frac{W_1 \left[ N + 1  \right] - 
\theta_r}{W_1^c - \theta_r}  \right)^{\frac{2}{B_p}+3} \label{q layer 1 to 2 VIC}
\end{equation}
Using this relationship VIC now models subsurface flow (flow in soil layer 2) as 
\begin{equation}
	Q_b'[N+1] = 
	\begin{cases}
\frac{D_s D_m}{W_s W_2^c}W_2^-[N+1], &  W_2^-\left[ N+1 \right] \in [0, W_s W_2^c] \\
\frac{D_sD_m}{W_sW_2^c}W_2^-\left[ N+1  \right]\\ + \left( D_M - \frac{D_sD_m}{W_s}  \right) 
\\ \cdot \left( \frac{W_2^-\left[ N+1  \right] - W_sW_2^c}{W_2^c - W_sW_2^c}  \right), 
& W_2^-\left[ N+1  \right] \geq W_sW_2^c
	\end{cases}
\end{equation}

Updating the lower layer soil moisture is then described as 
\begin{align}
    W_2^+\left[ N + 1 \right] &= \text{min}\left( W_2^- \left[N+1\right] \right. \nonumber
    \\& \left. +\left(Q_{12} \left[N+1\right]  - Q_b'\left[ N+1 \right] - E_2 \right)
    \cdot \Delta t, W_2^c\right).
\end{align}

At the edge case of $W_2^+ = W_2^c$, the second soil layer cannot absorb more 
water and all the excess streamflow is added to the runoff. An extra runoff term 
\begin{equation}
    Q_b''\left[ N + 1 \right] =  W_2^- \left[N+1\right]+\left(Q_{12}
    \left[N+1\right]  - Q_b'\left[ N+1 \right] - E_2 \right) \cdot \Delta t - W_2^c
\end{equation}
is therefore added.

The total streamflow is expressed as 
\begin{equation}
    Q_b = 
    \begin{cases}
        Q_b', & W_2^+ < W_2^c \\
        Q_b' + Q_b'', & W_2^+ = W_2^c
    \end{cases}
\end{equation}

For other ground covers than bare soil $Q\left[ 1...N \right]$ the model is slightly 
modified. We do not represent the details of this in this thesis and instead shift 
our focus to the next challenge of the VIC: Frozen soil and snow modelling.

While \cite{VIC} included a simple way to model snow which treated snow coverage as 
a special land coverage feature where it was assumed that all the land was covered 
by a uniformly divided snow layer, many updates later down the line by users of VIC 
have added to and improved these representations. The newest version of VIC \cite{VIC5} 
uses a modification from 1999 \cite{VICFrozenSoil} to model frozen soil.

The model is based on the heat equation written for frozen soil, which is defined 
as 
\begin{equation}
C_s\pdv{T}{t}=\pdv{}{z}\left( \kappa \pdv{T}{z} \right) + \rho_i L_f \left( \pdv{\theta_i}{t}\right).
\label{VIC heat equation}
\end{equation}
From (\ref{VIC heat equation}) \cite{VICFrozenSoil} then uses an explicit numerical 
finite difference scheme to solve this one dimensional partial differential 
equation. In numeric form it is easy to rewrite the equation as
\begin{align}
	C_s\frac{T_i^t - T_i^{t-1}}{\Delta t} &= \left( \frac{\kappa_{i+1}^t - 
	\kappa_{t-1}^t}{\alpha}\right) \left( \frac{T_{t+1}^t - T_{t-1}^2}{\alpha} 
	\right)\nonumber \\ &+ \kappa_i^t \left[ \frac{2T_i+1^t + 2T_{t-1}^t - 4T_i^t - 2\gamma 
	f'(z)}{\beta}  \right] \nonumber \\ &+ \rho_i L_f \frac{(\theta_i)_i^t - 
    (\theta_i)_i^{t-1}}{\Delta t}. \label{vic frozen soil pde}
\end{align}
The terms in (\ref{VIC heat equation}) and (\ref{vic frozen soil pde}) are as follows:
\begin{itemize}
    \item $C_S$: Soil volumetric heat capacity [J/(m$^3$K)]
    \item $T$: Temperature [K]
    \item $z$: Depth [m]
    \item $\kappa$: Soil thermal conductivity [W/(mK)]
    \item $\rho_i$: Ice density [kg/m$^3$]
    \item $L_f$: Latent heat of fusion (?) [kg/m$^3$]
    \item $\theta_i$: Ice fraction in soil [-]. When $\theta_i=1$ there is no frozen soil and the term $\rho_i L_f\left( \pdv{\theta_i}{t}\right) $ in (\ref{VIC heat equation}) is defined as zero (the gradient of 0 is 0).
    \item $\alpha$: $(\Delta z_1 + \Delta z_2)$
    \item $\gamma$: $(\Delta z_1 - \Delta z_2)$
    \item $(T_{i+1}^t - T_{i-1}^t) / \alpha$
\end{itemize}

Explain all terms. Try to understand what the next terms mean. Then argue why 
this model isn't ideal (it is constrained, not that physical, blah blah).
\subsection{National Water Model}
The National Water Model (NWA) is a process based hydrological model. For completeness 
we briefly describe what we deem the most important underlying physics in the model 
in this section.


\subsection{Drawbacks}
Several terms in the SAC-SMA model are purely parametrized, needing different parameters 
for each basin if we want to see acceptable results. This leads to a plethora of 
issues:
\begin{itemize}
    \item It is not possible for the model to generalize well over several basins. \citationneeded
    \item The terms in the model are often hard to interpret. This is acceptable for people merely interested in a well performing model in practice, but it is a large detriment to hydrologists, physicists and people from other practices who may want to use the model for scientific discovery.
    \item Coming from the above stated problem, it is also hard to use such a 
        complex and highly parametrized model to find out what information it lacks to be able to improve.
\end{itemize}
While there do exist more physically driven models than SAC-SMA \citationneeded, 
we use the SAC-SMA model as our example of a traditional model in this thesis because
the CAMELS dataset includes benchmarks of it \cite{CAMELS_US}. 
There is a likelihood that some of these even more physical models can also benefit 
from the analysis in this thesis, however, as they are also highly parameter 
driven. Finding a relationship between easily obtainable data such as the static 
features in \cite{CAMELS_US} and \cite{CAMELS_GB} and physical constants in 
hydrological models is something we and \citationneeded predict will be a very 
interesting research topic for quite some time, hopefully giving insight into the 
physics of hydrology as well as improving the prediction power of hydrological models.

One of the many aspects of the model that are not well understood is the melting 
of snow and how that melted water spreads via the ground \citationneeded. 

\section{Machine Learning}
We give a brief explanation of the basics in Machine Learning here for better context 
before elaborating on the LSTM model central to this thesis.
The term Machine Learning was coined in \citationneeded by .... 
Machine Learning is a type of frequentist approach to statistical analysis where 
one creates a statistical model, often with several million parameters and finds 
the value of each parameter that makes the model approximate the data in the most 
accurate manner. How to find these parameters and how they are used differ for each 
model type. 
\subsection{Linear regression}
In the simple case of the Ordinary Least Squares (OLS) model we have a model on the form 
\begin{equation}
\mathbf{\hat{y}} = \mathbf{\beta} \mathbf{X}
\label{OLS}
\end{equation}
This assumes that the outcome $\mathbf{\hat{y}}$ can we represented as a linear combination 
of some fitted parameters $\mathbf{\beta}$ and the input features $\mathbf{X}$.
The goal here is then to find the minimum of the mean squared error (MSE) of this.
The MSE is defined as 
\begin{equation}
MSE = |\mathbf{y} - \mathbf{\hat{y}}|^2
\label{MSE}
\end{equation}
Here $\mathbf{y}$ is the observed outcome, in many cases called the ground truth.
$\mathbf{\hat{y}}$ is the prediction made by (\ref{OLS}). The goal is to find the
$\mathbf{\beta}$ that minimizes (\ref{MSE}). For this there is an analytical solution 
as long as the matrix in (\ref{OLS}) is reversible. In other words: This can be solved
analytically as long as there are more data points than there are variables (features, inputs).
The solution to the equation can be written as 
\begin{equation}
\mathbf{\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{y}
\label{OLS solution}
\end{equation}
\subsection{Bias-Variance trade-off}
\label{Bias-Variance trade-off}
When training any kind of machine learning model, one usually divides the data 
into at least two parts: The training dataset and the testing dataset. The training 
dataset is for training a given model, while the testing dataset is to be kept separate 
from the training process so as to not make the performance metrics of the model too optimistic. 
By "optimistic" what is meant is that the error, for scalar values (\ref{MSE}) is 
lower on the data the model is trained on than on data the model has not seen under 
training. The function that is minimized under machine learning is called a 
cost function and while (\ref{MSE}) is very commonly used for scalar outcomes 
there exist many other cost functions all with different characteristics. \citationneeded
To explain 
why this is important we need to have a quick look at what is known as the 
bias variance trade-off.
In the case of the OLS model the MSE can be rewritten into four parts:
\begin{equation}
    MSE = \text{Bias}^2 + \text{Variance} + \sigma^2
    \label{Bias Variance Decomp}
\end{equation}
For a full derivation of this, see \cite{vijayakumar2007bias}. When selecting and 
configuring machine learning models this trade-off is essential. The following is 
a qualitative explanation of what each term in (\ref{Bias Variance Decomp})
represents:
\begin{itemize}
\item Bias: The bias is the part of the error that comes from a model's lack of complexity.  If one were to try and represent a non-linear system on the form of (\ref{OLS}) for instance one would struggle to model the more complex interactions between input and outcome.
\item Variance: In many ways this error is the opposite of bias. It comes from a given model having too much complexity. This could come from the model having too many parameters to train compared to how much data is available for training. 
\item $\sigma^2$: This is known as the irreducible error. It is the inherent error in the data that is used for training. The model cannot reduce the error below this value as it is independent from the model. To reduce this error one would have to gather more accurate data using better instruments for instance. 
\end{itemize}
The aim of training a machine learning model is to find the best tradeoff between model
complexity and stability in search of the minimum of (\ref{Bias Variance Decomp}).
This concept also generalizes to other types of modelling than machine learning. 
\begin{figure}
	\centering
	\includegraphics{{examples}/bias_variance_runoff.pdf}
	\caption{Illustration of the bias-variance tradeoff. The analogy to physics-
	constrained runoff models is included in the figure to give context for as to 
	why this is important for the problem in this thesis work. The concept and 
	function form is inspired by Figure ?? in \cite{elemstatlearn}.}
	\label{Bias Variance rainfall}
\end{figure}
In \cite{VICbench} the concept of model flexibility is mentioned. This is 
analogous to a model with high variance, making it able to predict well in more scenarios, 
but possibly losing performance (overfitting) and interpretability if taken too far. 
In traditional 
rainfall-runoff modelling we could call a purely process-driven model biased, while 
a more data driven conceptual model would have higher variance. The concept of 
the bias-variance tradeoff concept is illustrated  in Figure 
\ref{Bias Variance rainfall}.
\subsection{Gradient Descent}
While the simple case of linear regression has an easily attainable analytical solution 
as seen in (\ref{OLS solution}), more advanced models may not. \citationneeded 
A generic computational optimization algorithm known as Gradient Descent can therefore 
be beneficial to employ in such a case \cite{elemstatlearn} \cite{handson}. The 
fundamental concept of Gradient Descent is to calculate the gradient $\Delta F(\mathbf{x})$ 
of the loss function $F(\mathbf{x})$. In the case of regression this function would 
be the MSE as described in (\ref{MSE}) while the variable $\mathbf{x}$ would be 
the regression coefficients $\mathbf{\beta}$. After calculating $\Delta F(\mathbf{x})$ 
one then does a gradient descent step, which is defined as
\begin{equation}
	\mathbf{x}_{i+1} = \mathbf{x}_{i} - \lambda \Delta_x F(\mathbf{x_i}).
	\label{Gradient descent}
\end{equation}
Here the subscript $i$ denotes the epoch. An epoch is defined as the point when 
the optimization algorithm has seen all the data in your training set once. 
$\lambda$ is called the learning rate and is known as a hyperparameter. 
A hyperparameter is a model parameter that is not trained in the training process 
and has to be optimized in a different way. More on this in chapter \ref{Hyperparameters}
A problem with (\ref{Gradient descent}) is that one is never guaranteed to find the 
absolute minimum of $F(\mathbf{x})$, the algorithm often converges at local minima \cite{elemstatlearn}.
The solution to this is to introduce stochasticity to the optimization algorithm. 
Stochastic Gradient Descent is a modification of (\ref{Gradient descent}) which 
iterates over minibatches of your dataset instead of the entire dataset at once. 
This has two main benefits:
\begin{enumerate}
\item It adds stochasticity by shuffling the batches for each epoch.
\item It significantly reduces the amount of RAM needed to run the calculations on a computer.
\end{enumerate}
Introducing minibatches yields another hyperparameter in form of the batch size. 
This is the amount of datapoints to be used in each iterations. An iteration is 
defined as when the optimizer has seen all data points in a minibatch, and there are 
therefore several iterations in an epoch.
Adding more complexity to Stochastic Gradient Descent (SGD) we get to the optimizer 
known as Adaptive Moment EstimAtioN (Adam) \cite{adam}. The ADAM optimizer is 
an optimizer which uses minibatches and also what is known as a momentum based approach.
As a slight simplification from what's written in \cite{fysstkweek40} the ADAM 
algorithm in terms of equations looks like:
\begin{align}
    \mathbf{g}_i &= \Delta_x E(\bm{x}) \\
    \bm{m}_i &= \frac{\beta_1 \bm{m}_{i-1} + (1 - \beta_1)\bm{g}_i}{1 - \beta_1^i} \\
    \bm{s}_i &= \frac{\beta_2 \bm{s}_{i-1} + (1-\beta_2)\bm{g}^2_i}{1 - \beta_2^i} \\
    \bm{x}_{i+1} &= \bm{x}_i - \eta_i \frac{\bm{m}_i}{\sqrt{\bm{s}_i}+ \epsilon}
\end{align}
Following the same naming scheme as (\ref{Gradient descent}), $\bm{x}$ is the 
learning parameter of the model and the subscript $i$ denotes the epoch (except 
for $\beta_{1/2}^i$, where it means power). 
$\beta_1$ and $\beta_2$ are two constants usually set to $\beta_1 = 0.9$ and 
$\beta_2 = 0.999$. $\epsilon$ is a regularization constant to avoid numerical 
instability in the fraction and is usually $\epsilon=1E-8$\footnote{These values 
for $\beta_1$, $\beta_2$ and $\epsilon$ are mentioned in \cite{adam} and seem 
to be used in practice in most cases. They are also the default values of the 
ADAM implementation in the Machine Learning Framework Pytorch \cite{NEURIPS2019_9015}.}.
$\bm{m}$ and $\bm{s}$ are known as the first and second momentum of the gradient 
$\bm{g}$ respectively. The learning rate $\eta_i$\\ is here subscripted with the 
epoch $i$. This is to allow for non-constant learning rates which is often beneficial \citationneeded.
\footnote{Although perhaps less needed for the ADAM optimizer as one can tune the 
$\beta$-constants to achieve a similar effect.}
The benefits of the ADAM optimizer is that it has low memory requirement and it calculates 
individual learning rates for for different parameters of a learning algorithm. 
This makes the learning rate $\lambda$ a bit more ambiguous as it is weighed 
differently for each parameter.
\subsection{Neural Networks}
Previously we used Ordinary Least Squares to introduce simple concepts in Machine 
Learning. As the model we use to do our analysis in this thesis is a type of 
Recurrent Neural Network, we now give a short introduction to ordinary neural 
networks before working us up to describing more advanced models like Recurrent 
Neural Networks (RNNs).
A neural network is a non-linear machine learning model that can be used for both 
regression and classification. In this thesis we focus on the regression case and 
will therefore stick to describing that. The concept of the neural network was 
first described in \cite{rosenblatt1958perceptron}. 
\begin{figure}
    \input{figures/tikz/neural_network.tex}
    \caption{A multilayer perceptron model as described in \cite{rosenblatt1958perceptron}. Figure taken and modified from \cite{neuralnetfig}.}
    \label{Neural network figure}
\end{figure}
Mathematically, a neural network can be written in this form:
\begin{equation}
    y_i^l = f^l \left( \sum_{j=1}^{N_{l-1}}\omega_{ij}^l y^{l-1}_j+b^l_i  \right)
    \footnote{While the concept is cited to \cite{rosenblatt1958perceptron}, this 
    equation was taken from Morten Hjorth-Jensen's great lecture notes \cite{fysstkweek40}
    from the Universuty of Oslo course Fys-Stk4155 as this is the author's favorite 
    mathematical description of a neural network}
    \label{MLP equation}
\end{equation}
There are a lot of variables, subscripts and superscripts here, going through them 
systematically so as to not overwhelm the reader:
\begin{itemize}
    \item $l$ denotes what layer in the neural network we are in. Look at Figure 
    \ref{Neural network figure} to understand what is meant by a layer.
    \item $i$ denotes what "neuron" in layer $l$ we are looking at. 
    \item $y_i^l$ means the output of neuron $i$ in layer $l$. At the output layer $l=L$, it denotes the model output.
    \item $f^l$ is the activation function in layer $l$. An activation function is a function that is used to make the neural network non-linear. Some common activation functions are the Sigmoid function (\ref{sigmoid}) and $\tanh$. For the output layer $l=L$ the activation function is often different depending on whether we are doing classification or regression. For regression $f^L$ is often just $f^L(x)=x$.
    \item $\omega_{ij}^l$ is the weight corresponding to the output $y_j^{l-1}$ from neuron $j$ in layer $l-1$ when sent to neuron $i$ in layer $l$
    \item $b_i^l$ is known the bias term. It is a constant that is added to all inputs in neuron $i$ in layer $l$.
    \item The input to $f^l$ (the weighted sum plus the bias) can also be denoted as $z^l_j$
\end{itemize}
Figure \ref{Neural network figure} gives a visual representation of (\ref{MLP equation}).
The parameters $b$ and $\omega$ are trained using an optimizer. In the case of this 
thesis that would be the ADAM optimizer. To get gradients to use in the optimizer 
an algorithm known as backpropagation is used. Backpropagation is an algorithm that 
takes the gradient at the output layer $L$, which usually is easily derivable and 
uses the chain rule to find the gradient of the loss function with respect to all 
the weights and biases in the network.
We mention the sigmoid function, it is defined as
\begin{equation}
	\sigma(\bm{x}) = \frac{1}{1 + e^{-\bm{x}}} \label{sigmoid}
\end{equation}
For a normal feed forward neural network, the backpropagation algorithm can be written 
as 
\begin{align}
    \delta^L_j &= \dv{f^L(z)}{z}\left(z^L_j\right) \pdv{C}{y^L_j} \label{output_backprop}\\
    \delta^l_j &= \sum_k \delta^{l+1}_k \omega^{l+1}_{kj} \dv{f^l(z)}{z}\left(z^l_j\right) \cite{fysstkweek40} \label{backprop}
\end{align}
The first equation (\ref{output_backprop}) is for the special case of the output 
layer, where $l=L$. This needs to be calculated first for us to be able to use 
(\ref{backprop}) to calculate $\delta^l_j$ for the neurons in the rest of the layer. 
The term $\delta^l_j$ is then used to get gradients for the weights $\omega^l_j$ 
and biases $b^l_j$ in this manner:
\begin{align}
    \pdv{C}{\omega^l_j} &= \delta^l_j y^l_j \label{dw}\\
    \pdv{C}{b^l_j} &= \delta^l_j \label{db}
\end{align}
Applying an optimizer step on all weights and biases using \ref{dw} and \ref{db}
will then ideally make the model reduce the cost function of the training data.
There is more to this than that though, hyperparameters, model complexity (often 
referring to the amount of parameters. In the case of a neural network that would 
be the amount of nodes and layers) dictate how well the model can fit the training 
data. As we discuss in chapter \ref{Bias-Variance trade-off} we do not always want 
the model to perfectly predict the training data as that can make the model perform 
worse on data is has not seen during training. There is also the problem of the 
optimization algorithm not necessarily hitting a global minimum then converging. 
The parameter-loss space is a complex high dimensional space often with infinitely 
many local minima that can lead the model to a worse fit than would be possible 
otherwise. \citationneeded
\subsection{Recurrent Neural Networks}
\begin{figure}
    \input{figures/tikz/rnn.tex}
    \caption{Illustration of an RNN cell. The squared Tanh indicated a neural 
    network layer with tanh as the activation function. Lines meeting indicate 
    concatenation while lines splitting means copying.}
	\label{RNN figure}
\end{figure}
A normal feed forward neural network is only sufficient at learning the relationship 
between non-structured inputs and outputs. This means that it is not well suited 
for image analysis, where each pixel's position relative to other pixels is important. 
It also means that it is not sufficient for time series data where you ideally 
want a model than can sequentially run through the data and use information about 
the past to better predict the future.

A Recurrent Neural Network (RNN) is a neural network that takes timed inputs 
sequentially, changing modifying the "state" of the network while using the same 
parameters for all future time-steps. In simple terms the algorithm goes like this:
\begin{enumerate}
    \item The model takes an input $\bm{x_t}$
    \item Using trained parameters it updates a hidden state vector $\bm{h_t}$
    \item The models gives an output based on the parameters and the hidden state $\bm{y_t}$.
\end{enumerate}
For an ordinary RNN the state updates and outputs are defined as: 
\footnote{Note that this describes only a single RNN cell. When dealing with RNNs 
we often use the term "cell" instead of "layer". There is nothing stopping us from 
creating a model architecture with multiple RNN cells. Also, keep in mind that there 
are many ways of sending in inputs and getting out outputs from RNN cells. Here we 
only present what is relevant for the model setup in this thesis.}
\begin{align}
    \bm{h}_t &= \tanh{(\bm{\omega}_{hh}\bm{h}_{t-1} + \bm{\omega}_{xh}\bm{x}_t+\bm{b}_h)} \\
    \bm{y}_t &= \bm{\omega}_{hy}\bm{h}_t + \bm{b}_y \citationneeded (should probably cite IN5400) \label{RNN output}
\end{align}
Here the variables are as follows:
\begin{itemize}
    \item $\bm{y}_t$: The output vector (remember that there can be several output time series) at time step $t$.
    \item $\bm{h}_t$: The hidden state vector at time step $t$.
    \item $\bm{x}_t$: The input vector at time step $t$.
    \item $\bm{\omega}_{hh}$: The weight vector used for the previous hidden state to affect the new.
    \item $\bm{\omega}_{xh}$: The weight vector that decides how much the input $\bm{x}_t$ affects the state of the RNN.
    \item $\bm{\omega}_{hy}$: The weight vector that decides how much the hidden state $h_t$ affects the output $y_t$.
    \item $\bm{b}_h$: A bias term that adds constant change to the hidden state.
    \item $\bm{b}_y$: A bias term used when calculating the next time step $y_t$.
\end{itemize}

In Figure \ref{RNN figure} we see that the output is represented as equal to the 
hidden state. While not illustrated in the figure, usually the way to get the output 
on the form we are interested involved feeding the hidden state through a neural 
network layer. This is what is done in (\ref{RNN output}).

The backpropagation algorithm for an RNN is quite a bit different although the 
underlying concept is the same. We argue it is not necessary to include the algorithm 
in this thesis but if the reader is interested they should read this source \cite{BPTT}.
The important takeaway from the backpropagation algorithm used for RNNs (called 
Backpropagation Through Time), is that it uses high amounts of memory because 
it needs to store values for every time step to be able to get proper gradients 
for the model parameters. Because of this, it is common to instead use an algorithm 
called Trunctated Backpropagation Through Time, where one stops saving values after 
a pre-defined time sequence. This saves memory but makes the model unable to learn 
dependencies further in time than the cutoff. It also leads to the introduction of 
a new hyperparameter which we will refer to as the \textbf{sequence length}. \citationneeded.

\subsection{Long Short-Term Memory}
\label{LSTM Theory}
The Long Short-Term Memory (LSTM) model is the type of RNN we employ in all our 
experiments in this thesis work. In this section we therefore also spend some 
time arguing why the way an LSTM model works fits the Physics we wish to simulate 
and is not just chosen by random. 

The LSTM is a more advanced type of recurrent neural network that exists because 
of a major drawback with the ordinary RNN. An RNN as described in the previous section 
struggles to learn long-term dependencies as the Backpropagation Through Time 
algorithm starts to downplay the importance of previous time-steps the further one 
advances in time. The fenomenon that causes this is known as the vanishing gradient 
problem \cite{bengio1994learning} \cite{graves2012long} \footnote{Mentions of 
vanishing and exploding gradients are common in Machine Learning and are not 
exclusive to Recurrent Neural Networks. Historically the popularity of activation 
functions in normal neural networks has also been dictated by these issues. The 
Sigmoid function (\ref{sigmoid}) can lead to vanishing gradients as the gradient 
approaches $0$ when $x\rightarrow \infty$.}.
The LSTM model is designed to fix this, 
hence the name. This is the reason we use the LSTM model and not a normal RNN in 
this thesis even though an RNN would be less computationally expensive to train, 
many of the dependencies in traditional rainfall-runoff modelling are long term. 
The most obvious case is the modelling of snow. Not the melting itself, but for 
the model to remember that snow accumulation can lead to lower discharge when it 
happens and more discharge once the snow starts to melt.

As opposed to the case of the vanilla RNN, trunctating the Backpropagation Through 
Time algorithm actually doesn't break long-term dependencies in an LSTM as the 
model architecture itself is designed to keep track of these \cite{graves2012long}.
This means that the LSTM has two clear advantages:
\begin{enumerate}
	\item An LSTM does not suffer from the same vanishing gradient problem that the RNN does.
	\item An LSTM is less sensitive to trunctating the Backpropagation Through Time algorithm.
\end{enumerate}

\begin{figure}
\centering
    \input{figures/tikz/lstm.tex}
\caption{A single basic LSTM cell as proposed in \cite{hochreiter1997long}. Visually we see the point of the Cell State $C$ here is to not retain more information 
by being affected less over time than the hidden state $h$. This is how the LSTM 
avoids the gradient issues regarding long term dependencies that the RNN suffers 
from.
The Tikz code for creating this figure is released under the permissive Beerware license 
and is a modified version of the figure in \cite{tikzlstm}.
}
\label{LSTM figure}
\end{figure}
To understand how an ordinary LSTM works we need to have a look at Figure \ref{LSTM figure}.
In the figure lines meeting means concatenation, the boxes represent neural network layers. The boxed $\sigma$ for instance indicates a neural network layer with the Sigmoid function 
as the activation function. Circles indicate a pointwise operation.
The biggest mathematical difference between a normal RNN and an LSTM is the introduction 
of the cell state $\bm{c}_t$. The cell state is only affected twice per time step 
by relatively simple operations (pointwise multiplication and addition). This 
ensures that the gradients related to the cell state get simplified and therefore 
suffer less from the vanishing gradient effect that hinders ordinary RNNs from 
learning long term dependencies. The three connections between the hidden state 
$\bm{h}$ and cell state $\bm{c}$ are often called "gates".
\begin{enumerate}
    \item The first (from left to right) is often called the "forget gate". This is because $\sigma(x) \in \left<0,1\right>$, making the forget gate either not affect the hidden state at all or lower the values in the $\bm{c}_{t-1}$-vector.
    \item The second gate is called the "input gate". This is where new information is added to the cell state through multiplying the output from two neural network layers, one with Sigmoid activation and one with $\tanh$ activation and then adding that product to the cell state.
    \item The third gate is called the "output gate" and it dictates the flow of information from the cell state to the hidden state. It is defined as the pointwise product of $\tanh{\bm{c}_t}$ and the output of the concatenated previous hidden state and current input $\left[\bm{h}_{t-1},\bm{x}_t\right]$. The output of this gate is both used as the updated hidden state $\bm{h}_t$ and as the output\footnote{At least in the model configuration we use, there are many variants of the LSTM model \cite{graves2012long}.}.
\end{enumerate}
Now that we have a qualitative explanation of the LSTM model, we lay it down in 
mathematical terms as well:
\begin{align}
    \bm{f}_t &= \sigma\left(\bm{\omega}_f\cdot \left[\bm{h}_{t-1},\bm{x}_t\right]+\bm{b}_f\right) \label{forget gate}\\
    \bm{i}_t &= \sigma\left(\bm{\omega}_i \left[ \bm{h}_{t-1}, \bm{x}_t\right] + \bm{b}_i\right) \circ \tanh\left(\bm{\omega}_{ii}\left[ \bm{h}_{t-1},\bm{x}_t  \right] + \bm{b}_{ii}\right) \label{input gate}\\
    \bm{c}_t &= \bm{c}_{t-1} \circ \bm{f}_t + \bm{i}_t \\
    \bm{h}_t &= \sigma \left( \bm{\omega}_o \left[ \bm{h}_{t-1}, \bm{x}_t \right] 
    + \bm{b}_o \right) \circ \tanh(\bm{c}_t) \label{output gate}
    \cite{hochreiter1997long} \cite{gers1999learning}
\end{align}
The terms are as follows:
\begin{itemize}
    \item $\sigma$: The Sigmoid function (\ref{sigmoid}).
    \item $\bm{f}_t$: Forget gate at time step $t$.
    \item $\bm{\omega}_i$ and $\bm{f}_i$: Neural network weights and bias in the forget gate.
    \item $\circ$: Hadamard product.
    \item $\bm{i}_t$: Input gate at time step $t$.
    \item $\bm{\omega}_i$ and $\bm_i$: Neural network weights and bias in the Sigmoid activated neural network in the input gate.
    \item $\bm{\omega}_{ii}$ and $\bm_{ii}$: Neural network weights and bias in the tanh activated neural network in the input gate.
    \item $\bm{c}_t$: Cell state at time step $t$.
    \item $\bm{h}_t$: Hidden state at time step $t$.
    \item $\bm{\omega}_o$ and $\bm{b}_o$: Neural network weights and bias in the output gate.
    \item $\bm{x}_t$: Input vector at time step $t$.
\end{itemize}
There are several ways to use an LSTM for prediction. The method relevant for this 
thesis is the one we present here.
To predict the output $\bm{y}_t$ at time step $t$:
\begin{enumerate}
    \item Take a subset of the input vector $\bm{x}_{t-\text{sequence length}:t}$.
    \item Feed it through the LSTM cell, saving the hidden vector $\bm{h}_t$.
    \item Feed $\bm{h}_t$ through an ordinary neural network layer. This is essentially to do a weighted sum of the elements in $\bm{h}_t$ so that we get an output vector of the same dimensions as the output $\bm{y}_t$. With only one output as with rainfall-runoff modeling this means summing all the values $\bm{h}_t$ into a single scalar $y_t$.
\end{enumerate}
\subsection{Implementing static features along with time series}
The description of the LSTM model we give in the above section only contains information 
for how to implement time series, not static features such "area" that do not vary 
with time. There are multiple ways to implement this. An obvious choice here is to 
naively treat each static feature as a time series that does not change. This seems 
counter-intuitive as it makes the model not see any difference between time series and 
static features, but it is very easily implementable and memory-wize does not affect 
the training algorithm too much\footnote{Memory is often the bottleneck of training 
machine learning models. \citationneeded}. This way we train the exact same type 
of model and do not need to modify anything.

Recently a small modification to the LSTM model was proposed for this purpose. 
It is called the Entity Aware LSTM(EA-LSTM) \cite{lstm_second_paper}. The paper 
indicates that this modified LSTM performs worse than the naive approach, going 
from a median NSE of 0.76 to 0.74 on the CAMELS dataset \cite{CAMELS_US}. The 
underlying philosophy of this thesis however still makes us believe that this is 
the model we should employ, as it is the most interpretable model \cite{lstm_second_paper} 
and the goal of this thesis is to get a better understanding of the underlying 
physics in rainfall-runoff modelling that these static features can help explain.

Mathematically, an EA-LSTM is very similar to an LSTM. We only need to change the 
input gate described in (\ref{input gate}): 
\begin{equation}
    \bm{i}_t = \sigma\left(\bm{\omega}_i  \bm{x}_s + \bm{b}_i\right) \circ \tanh\left(\bm{\omega}_{ii}\left[ \bm{h}_{t-1},\bm{x}_t  \right] + \bm{b}_{ii}\right) \label{EA-LSTM input gate}\\
\end{equation}
This way the part of the input gate is affected by the static features, making 
the static features able to affect the cell state. This makes the model able to 
change how it views long-term dependencies based on information from the static features. 
The neuron activation $\sigma (\bm{\omega}_i \bm{x}_s + \bm{b}_i)$ is a metric that 
is of interest as it is likely the key to understanding how the model interprets 
the physical information of the data\footnote{Although we are of course aware that 
the model may not at all use any physical information from the static features, but 
instead just learns that basins with similar static features behave similarly.}. 


\subsection{Hyperparameters}
\label{Hyperparameters}
Is this section actually needed?
There are mentions of a concept known as hyperparameters in this chapter. A hyperparameter 
is a parameter that is not trained as part of the optimization algorithm. For an 
LSTM the most relevant hyperparameters are:
\begin{itemize}
    \item Learning rate
    \item Batch size
    \item Sequence length
    \item Dropout rate
    \item Initial forget gate bias
    \item Gradient norm clip threshold
\end{itemize}
\subsection{Feature selection}
\subsubsection{Permutation test}
\label{Feature selection}
One of the criticisms of machine learning methods
is that they are not easily interpretable. This 
is especially true if one wants to train a model 
on a dataset with an overwhelming amount of 
features. There are several strategies for 
selecting a smaller subset of features in a dataset.
The method we briefly describe here is called the 
permutation test:
Given a feature $j$, the permutation importance 
$i_j$ is equal to 
\begin{equation}
i_j = s - \frac{1}{K} \sum_{k=1}^K s_{k,j}\quad \cite{permutation} \cite{breiman2001random}.
\label{Permutation equation}
\end{equation}
Here $K$ denotes how many permutations we average over for each feature, $s$ is
the model's score on the original data and $s_{k,j}$ is the score of permutation 
number $k$ of the feature $j$. In essence (\ref{Permutation equation}) describes 
how much the performance of a model varies when "destroying" the information 
contained in a feature, therefore explaining the importance of the feature 
according to the current model. It is then important to remember that this is not 
the true importance of the feature, only the importance the model thinks the feature 
has. The scoring method $s$ can be any model scoring statistic, often the $R^2$
score for regression.
A major problem for this method is that (\ref{Permutation equation}) could give 
unrealistically low significance to features that are highly correlated to other 
features. In this case the feature may very well be important, but the information 
contained in it is also contained in one or more other correlated features, meaning 
the model doesn't lose as much information as one may think. This would be fine 
if we were just looking to simplify a well performing model, but what we aim for 
is improved interpretability so that we may know which features are actually important 
to the physical system.

A way around this is to first remove unneeded correlated features so that the model 
contains as little duplicate information as possible. Reducing the amount of 
features also has the added benefit of reducing the model variance because of 
decreased complexity and improving model interpretability as getting an intuitive 
understanding of what a model needs to properly approximate a system is easier when 
there are fewer features.

\begin{figure}
	\includegraphics{{examples}/cluster_example.pdf}
    \caption{Example of a hierarchical plot created. The ticks on the x-axis 
    represent $f(x)$ for $x \in \left[0,100\right]$}
    \label{wald example}
\end{figure}
A method to remove correlated features that is fairly simple to use is hierarchical 
clustering on the correlation of the features in a dataset. 
The most well known algorithm for finding correlation is called Pearson correlation. It has 
the drawback of only finding linear correlation. Spearman correlation on the other 
hand is more general and can find non-linear correlation \cite{spearman}. After 
finding the correlations of the features one then uses a clustering method to 
visualize these correlations. The source at \cite{permutation_clustering} suggests 
using hierarchical clustering and choosing a single feature from each branch below 
a chosen threshold. For illustrational purposes we include such a clustering figure
created on synthetic data, it can be seen in figure \ref{wald example}.
While \cite{permutation_clustering} suggests using the Spearman rank order correlation,
we instead choose to use the Pearson correlation as it is more easily interpretable.
Also, when removing features we argue that it should not matter whether two variables 
are strongly positively or negatively correlated, so we choose to do hierarchical 
clustering based on the absolute value of the Pearson correlation.
The idea here is to set a level at the y-axis of the plot, each branch that is
"cut off" will then only get to contribute one single feature, discarding all others.
For instance: In figure \ref{wald example} we may want to set the level at 0.1. 
This would lead to only 4 features being included. This makes sense as we 
for instance do not need $2x$, $x$ and $-x$ in the same model, those features 
represent the same information. A problem with this method is that there is no 
automatic way to determine which features to actually choose. Often this is done at 
random, but it could for instance be an idea to use some existing domain knowledge 
for this decision.

\subsection{Addressing common criticisms of Machine Learning}
There are several common criticisms of machine learning and other data driven 
approaches to physical modelling. Two common ones are:
\begin{enumerate}
    \item Machine learning models are hard to analyze as they essentially are just a collection of arbitrary parameters that are optimized based on a dataset. It is therefore difficult to say whether a machine learning model actually "learns" any physics\footnote{Though this is a question of definition. What does "learning physics"actually mean?}.
    \item Machine learning models are unable to include all the knowledge already present in a scientific field.
\end{enumerate}
We believe both these criticisms are valid and need to be addressed. First off: 
While analyzing machine learning models can be very difficult, that is not necessarily 
always the case. It depends greatly on how complex the model is how much different 
data the model uses among other things. A linear regression model is easy to interpret, 
but it becomes much less easy to interpret if it needs several thousand different 
inputs \cite{elemstatlearn}. This is the reason we employ feature selection here 
in this thesis. Our goal is not to get the highest performing model, but rather 
to use the model as a tool for physical discovery. This also addresses point 2: 
We  are in fact not looking to use LSTM models as replacements for physical models, 
but are instead interested in improving existing models like SAC-SMA \citationneeded 
by seeing if there is any possibility for improving the sometimes lacking physical 
modelling of classical models. It is important to remember that the best performing 
classical rainfall-runoff models also include lots of trained/optimized parameters 
that do not necessarily represent any physical phenomena. We agree with \cite{hybrid_paper} 
that the power of machine learning when it comes to leveraging data should be 
used in a way that improves our understanding of Physics.
