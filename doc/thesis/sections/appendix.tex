\section{CamelsML documentation}
\label{camelsml documentation}
\subsection{Installation REMEMBER TO ADD TO PYPI, IF NOT, CHANGE THE INSTALLATION INSTRUCTION!!!}
Requirements:
\begin{itemize}
    \item Cuda if using an Nvidia GPU. ROCm should in theory also work if on a AMD Radeon GPU, but needs a specialized version of Pytorch called Pytorch-ROCm. The author has not been successful when trying to run this on an AMD RX 6800 XT.
    \item GNU/Linux (Other operating systems have not been tested)
    \item Python $>=$ 3.6
    \item It is also possible to run machine learning algorithms on CPUs, therefore bypassing the need for CUDA or equivalents, but this is not recommended as it is significantly slower and computationally inefficient to do so.
\end{itemize}
In the terminal, run:
\mint{bash}|pip install camelsml|
NB: The author recommends using a dependency resolver better suited than pip. 
Pipenv (\url{https://pypi.org/project/pipenv/}) is what is being used in this thesis.
For a Pipfile that is confirmed to work on Ubuntu $>=$ 20.04 and an updated installation of Arch Linux at the time this thesis was submitted, 
see the root of the github repository for this thesis at \url{https://github.com/bernharl/Master-Thesis}.
\section{Usage}
Here we show a minimum example of how to train an LSTM with CamelsML. 
\begin{listing}
\begin{pythoncode}
from camelsml import load_config, train

cfg = load_config(cfg_file="run_config.txt", device="cpu", num_workers=24)
train(cfg)
\end{pythoncode}
    \caption{Minimal running example of CamelsML}
\end{listing}
As seen here, the CamelsML package needs a variable called "cfg", which is a dictionary 
containing the model configuration, how the train-validation-test split is defined, 
and so on. 
A simple example of a configuration file could be this, which trains an ordinary 
LSTM for 30 epochs using a batch size of 1024 is shown below:
\begin{listing}
\begin{cfgcode}
run_dir: <path> # Folder to save runs in
camels_root: <path> # Root folder of dataset
train_start: 01101971 # Date to start training period of timeseries
train_end: 30092015 # Date to end training period of timeseries
val_start: 01101971 # Date to start validation period of timeseries
val_end: 30092015 # Date to end validation period of timeseries
epochs: 30 # Number of epochs
learning_rate: 1e-3 # Initial learning rate
seq_length: 270 # Sequence length
batch_size: 1024 # Batch size
hidden_size:  256 # Amount of nodes in neural network layers in the LSTM gates
initial_forget_gate_bias: 5 
log_interval: 50 # How often to log
clip_norm: True # Whether to clip gradients
clip_value: 1 # Max of gradient norm
dropout: 0 # Dropout rate
seed: 19970204 # Seed, for reproducability 
cache_data: False # Whether to cache all training data in RAM
no_static: True # No static features
evaluate_on_epoch: True # Run evaluation after each epoch
train_basin_file: <path> # Plain text list of basins to use for training 
val_basin_file:  <path> # Plain text list of basins to use for validation
test_basin_file: <path> # Plain text list of basins to use for testing
\end{cfgcode}
\caption{Example configuration file of an LSTM model trained on CAMELS-GB without using static features.}
\end{listing}
For more examples, all the models trained in this thesis along with scripts for 
setting up train-test splits and cross validation are contained in the "runs" directory at the 
Github page for this thesis (\url{https://github.com/bernharl/Master-Thesis/tree/master/runs}).
If you are reading this some time after the thesis is submitted, there may be updated 
documentation for CamelsML on the Github page (\url{https://github.com/bernharl/ealstm_regional_modeling_camels_gb})
