In this chapter we present the results relevant to our discussed topics in the 
next chapter. To keep this chapter structured in a way that makes it easy to 
look up we divide our results into five main sections presenting model importance 
and feature importance for four cases:
\begin{enumerate}
    \item Models trained on CAMELS-GB \citep{CAMELS_GB}.
    \item Models trained on CAMELS \citep{CAMELS_US} in additiion to traditional 
        models provided by \citationneeded.
    \item Models trained on a dataset comprised of both CAMELS and CAMELS-GB.
    \item Models trained on CAMELS and validated on CAMELS-GB and vice-versa.
    \item Models refit on the full train set and valiated on the test set not 
        touched in sections 1 - 4.
\end{enumerate}

\section{Models trained on CAMELS-GB}
In this section we present the results related to model selection and feature 
importance when training and predicting on CAMELS-GB \citep{CAMELS_GB}. To act 
as a proof of concept of the feature importance method described in Chapter 
\ref{Feature selection} we also include the performance and feature importance of 
a model trained using dataset a (see Table \ref{attribute table}) in addition to
static attributes directly derived from the observed outcome.

\subsection{Performance}
\begin{figure}
    \centering
    \includegraphics{{results_section/camels_gb/cdf_val}.pdf}
    \caption{Cumulative distribution function of the NSE score of LSTM models trained 
    on CAMELS-GB \citep{CAMELS_GB}. "Overfit model" is a model deliberately trained 
    using static basin attributes derived from the runoff time series of the basins. 
    The other models are described in Table \ref{all models}.}
    \label{CAMELS-GB CDF validation}
\end{figure}
Figure \ref{CAMELS-GB CDF validation} shows the performance of several LSTM models 
trained on CAMELS-GB \citep{CAMELS_GB}. The overfit model is trained on dataset a 
in Table \ref{attribute table} in addition to attributes directly derived from the 
observed outcome.
The overfit model significantly outperforms all other models.
 Of the other models there seems to be generally two levels of 
performance. The models using attribute subsets a and b perform similarly, although 
in the favour of dataset a, while 
the models trained with no attributes in general perform worse. Enabling dropout 
slightly decreases performance.
There seems to be little overall difference in performance between EA-LSTM and 
LSTM models, apart from $GB_{lstm, all}$ which performs with a negative NSE value 
for a smaller fraction of the basins.
 All other ungauged  models perform with a negative NSE value for 
approximately 10\% of the basins.

\begin{figure}
    \centering
    \includegraphics{results_section/camels_gb/prediction_best_all_concat_seq_len_cv.pdf}
    \caption{Highest scoring prediction on the validation set  made using 
    GB$_\text{lstm, all}$ compared to the observed outcome. "Obs" is observed runoff, 
    "Sim" is predicted runoff. The basin data is taken from CAMELS-GB \citep{CAMELS_GB}.}
    \label{best prediction all concat seq len}
\end{figure}
The highest scoring validation set prediction made using GB$_\text{lstm, all}$ is 
shown in Figure \ref{best prediction all concat seq len}. It has an NSE score of $\approx 0.96$.
\subsection{Importance}
\begin{table}
    \centering
    \caption{Top 20 (ranked by median importance) attributes of the overfit model described earlier 
    in this section according to the permutation test. The columns are percentiles.}
    \input{{tables/results_section/overfit_importance}.tex}
    \label{overfit table}
\end{table}
Table \ref{overfit table} shows 5th, 25th, 50th, 75th and 95th percentiles of the 
importances of the top 20 basin attributes of the overfit model according to the 
permutation test described in Chapter \ref{Feature selection}. We observe that the 
attribute with the highest importance in all quantiles is Q95, which is the 
95th percentile runoff derived from the observed runoff time series. Q95 has an 
importance of at least 0.05 for 75\% of the basins in the training set. At the 
second spot we have baseflow\_index\_ceh. This attribute has median and 25th percentile 
importances similar to those of Q95, but is significantly less important for 25\% 
of the basins in the training set.
\begin{table}
    \centering
    \caption{Top 20 (ranked by median importance) attributes of GB$_\text{lstm, all}$ 
    according to the permutation test. The columns are percentiles.}
    \input{{tables/results_section/all_features_concat_seq_len_importance}.tex}
    \label{all concat table}
\end{table}

The top 20 importances found by US$_\text{lstm, all}$ (cross validated) using the permutation test 
are shown in Table \ref{all concat table}. The top two features are low\_prec\_dur 
and low\_prec\_freq, these have significantly higher 75th percentile and upward 
importances than all other attributes. The third most important attribute is 
tawc and has a 75th percentile importance that is roughly half of the above ranked 
attribute. The highest ranked attribute has an importance of 0.09 for 25\% of the 
basins in the training set. Of these 20 attributes seven (low\_prec\_dur, low\_prec\_frec, 
high\_prec\_frec, p\_seasonality, frac\_snow, p\_mean, high\_prec\_dur) are climatic indices 
derived from the time series the model is trained on. Aridity is also a climatic indice, 
but is not based on any time series accessed by the model. Three attributes (grass\_perc, 
crop\_perc, inwater\_perc) are land cover attributes. Three attribtues (tawc, 
root\_depth, conductivity\_hypres) are soil attributes. Four attributes (elev\_90,
elev\_10, elev\_50, area) are topographic. Two attributs (no\_gw\_perc and 
inter\_mod\_perc) are hydrogeologic attributes. Details of these attributes 
can be found in Table 2 in \citet{CAMELS_GB} and sources therein.

\section{Models trained on CAMELS}
In this section we present the results related to model selection and feature 
importance when training and predicting on CAMELS \citep{CAMELS_US}. This section 
is meant to show whether we are able to produce results similar to the those of 
\citet{lstm_third_paper} in a way that is comparable to the rest of our experiments.
In addition we present apparent feature importance of these trained models.
\subsection{Performance}
\begin{figure}
    \centering
    \includegraphics{{results_section/camels_us/cdf_val}.pdf}
    \caption[lol]{Cumulative distribution function of the NSE score of models trained 
    on CAMELS \citep{CAMELS_US}. "VIC" is the Variable Infiltration Capacity model 
    calibrated on CAMELS. "SAC-SMA" is the SACramento Soil Moisture Accounting 
    model calibrated on CAMELS. 
    These benchmarks are provided by \citet{CAMELS_hydroshare} and are originally 
    created by \citet{VICbench} and are trained per-basin as opposed to the other 
    models.
    "NWM" is a benchmark of the National Water Model run on CAMELS. (CHECK IF PER-BASIN, 
    I THINK ITS UNGAUGED!!!). This benchmark is available without any licensing at 
    \citet{NWMbench} and we use an already preprocessed version provided by 
    \citet{lstm_third_paper}.
    The other models are described in Table \ref{all models} and are based on the 
    models originally trained by \citet{lstm_third_paper} but retrained for better 
    comparison with the other LSTM models in this thesis.}
    \label{CAMELS-US CDF validation}
\end{figure}
Figure \ref{CAMELS-US CDF validation} shows the performance of our reimplementation 
of the model created by \citet{lstm_third_paper} along with three traditional 
hydrological models. The results here indicate that the process-driven models 
VIC and NWM perform similarly, SAC-SMA and an LSTM without basin attributes perform 
similarly and better than the process-driven model and that there is a clear difference 
in performance in favour of US$_\text{Kratzert}$. The LSTM models both perform 
with an NSE value below zero for approximately five percent of the basins used 
for validation. 
\begin{figure}
    \centering
    \includegraphics{results_section/camels_us/prediction_best_kratzert.pdf}
    \caption{Highest scoring prediction on the validation set  made using 
    US$_\text{Kratzert}$ compared to the observed outcome. "Obs" is observed runoff, 
    "Sim" is predicted runoff. The basin data is taken from CAMELS \citep{CAMELS_US}.}
    \label{best prediction kratzert}
\end{figure}

The highest scoring validation set prediction made using US$_\text{Kratzert}$ is 
shown in Figure \ref{best prediction kratzert}. It has an NSE score of $\approx 0.92$.
\subsection{Importance}
\begin{table}
    \centering
    \caption{Permutation importance of all static basin attributes used by 
    US$_\text{Kratzert}$ (same model configuration as in \citet{lstm_third_paper} 
    refit to the cross validation split used in this thesis), ranked by median 
    importance. ADD MORE INFO ONCE RUN IS DONE!!!}
    \input{{tables/results_section/kratzert_features_importance}.tex}
    \label{kratzert importance}
\end{table}
The permutation importances of the static basin attributes used by US$_\text{Kratzert}$ 
ranked by median importance are shown in Table \ref{kratzert importance}.
\section{Models trained on CAMELS and CAMELS-GB}
In this section we present results related to model selection and feature importance 
when training and validating using both CAMELS and CAMELS-GB as a combined dataset.
This section is meant to show whether we can obtain satisfactory performance on 
ungauged basins from both datasets with the same model. 

\subsection{Performance}
\begin{figure}
    \centering
    \includegraphics{{results_section/mixed/cdf_val}.pdf}
    \caption{Cumulative distribution function of the NSE score of LSTM models trained 
    on a dataset consisting of both CAMELS \citet{CAMELS_US} and CAMELS-GB \citep{CAMELS_GB}. 
    The models are described in Table \ref{all models}. The top figure shows the 
    performance of the models on CAMELS-GB and the bottom figure shows the performance 
    on CAMELS. The top figure is the cross validated perforamnce on CAMELS-GB, the 
    bottom figure on CAMELS.}
    \label{mixed CDF validation}
\end{figure}
Figure \ref{mixed CDF validation} shows the cross validated performance of three 
LSTM models trained on a dataset consisting of both CAMELS and CAMELS-GB. 
On CAMELS-GB the models with basin attributes perform significantly better than 
the model trained without them. This also applies to the performance on CAMELS, 
but here the difference is smaller. The EA-LSTM and LSTM models with basin attributes 
perform similarly, having a median NSE of $~0.77$ on CAMELS-GB and $~0.65$ on CAMELS.
Compared to the best performing models in Figure \ref{CAMELS-GB CDF validation} and \ref{CAMELS-US CDF validation} 
the performance of the mixed model is comparable, although lower.
\subsection{Importance}
\section{Models trained for transfer learning between CAMELS and CAMELS-GB}
In this section we present results related to model selection and feature importance 
when training on CAMELS-GB and validating on CAMELS and vice-versa. This section 
is meant to show whether we can use information in one dataset to be able to 
satisfyingly make predictions on the other.
\subsection{Performance}
\begin{figure}
    \centering
    \includegraphics{{results_section/transfer/cdf_val}.pdf}
    \caption{Cumulative distribution function of the NSE score of LSTM models trained 
    on CAMELS \citep{CAMELS_US} and validated on a validation part of CAMELS as well as 
    the entirety of CAMELS-GB \citep{CAMELS_GB}. The top figure shows the 
    performance of the models on CAMELS-GB and the bottom figure shows the performance 
    on CAMELS.}
    \label{transfer CDF validation}
\end{figure}
\begin{figure}
\includegraphics{{results_section/transfer/training_progress}.pdf}
\caption{Training performance per epoch of Transfer$_\text{US, none}$ and Transfer$_\text{GB, none}$ 
on CAMELS-GB (top) and CAMELS (bottom). The orange line indicates the median NSE, 
the green line indicates the mean NSE.}
\label{training progress transfer}
\end{figure}
Figure \ref{transfer CDF validation} shows models trained on CAMELS and validated on 
CAMELS-GB and vice-versa. No models trained on CAMELS-GB perform at a satisfactory level 
on basins from CAMELS. The models trained on CAMELS all perform at a lower level 
on CAMELS-GB than models that are trained on CAMELS-GB and the best performing 
transfer model is Transfer$_\text{US, none}$. This model has no access to static 
basin attributes. The transfer models also perform worse on the validation part of 
the dataset they are trained on as the chosen epoch is based on the performance 
on both datasets. This is illustrated in Figure \ref{training progress transfer}, 
which shows boxplots of the performance per epoch of Transfer$_\text{US, none}$ and Transfer$_\text{GB, none}$ 
per training epoch on both CAMELS-GB (top) and CAMELS (bottom). We observe that 
the performance of Transfer$_\text{US, none}$ increases on CAMELS-GB up until 
epoch 5 and then gradually decreases while it increases steadily and converges 
on CAMELS around epoch 10. The epoch optimized for both datasets which is shown in 
Figure \ref{transfer CDF validation} is epoch 9. For Transfer$_\text{GB, none}$ 
this is more apparent. Epoch 1 is the best epoch for the performance on CAMELS, 
while the performance on CAMELS-GB seems to increase for every epoch. The optimized 
epoch found for this model on both datasets is epoch 24.
\subsection{Importance}
\section{Test performance of best models}
In this section we present test results from the models we choose from Section 1 - 4 
in this chapter and compare with the cross validation performance. The test 
performance is meant to measure real world performance and not to compare performance 
of models to do model selection.
\begin{figure}
\centering
\includegraphics{{results_section/camels_gb/cdf_test}.pdf}
    \caption{GB$_\text{lstm, all}$ refit on the full dataset and validated on the 
    test set compared with the cross validated performance on the train set.}
\label{test gb}
\end{figure}
\begin{figure}
    \centering
    \includegraphics{{results_section/camels_us/cdf_test}.pdf}
    \caption{US$_\text{Kratzert}$ refit on the full dataset and validated on the 
    test set compared with the cross validated performance on the train set.}
    \label{test us}
\end{figure}
Figure \ref{test gb} shows GB$_\text{lstm, all}$ refit on the full training set, 
trained for the optimal amount of epochs found via cross validation and validated 
on the test set. This is compared to the cross validated performance of the same 
model. We observe that cross validated and testing performance are similar.

Figure \ref{test us} shows US$_\text{Kratzert}$ refit on the full training set 
compared to the cross validated performance. The performance is similar, although 
notably lower between the 40th and 70th percentile performant basins.
