\section{Summary}
We have shown that LSTM models perform well on CAMELS-GB as well as CAMELS, and 
also that a single model can perform well on both datasets at the same time, even 
with less than optimal preprocessing of the mixed dataset.

Training on CAMELS and validating on CAMELS-GB and vise-versa yielded disappointing, 
though not surprising results. We deemed this to be because the datasets cover 
domains that are too different from one another, as well as lacking equivalent 
attributes.

Our static attribute analysis yielded no obvious ways to improve the performance 
of process-driven models, as our models preferred static attributes containing 
information already used by process-driven models. Improving the performance of and 
physical understanding provided by process-driven models is a difficult task.

LSTM models seem to have the potential to both outperform and improve traditional 
models. In the future we hope this leads to both improved physical understanding, 
as well as model quality, in several fields of the physical sciences. Hydrological 
systems are highly complex, and the need to use large scale data analysis to 
obtain a better understanding of the inner physical processes has yet to be 
fulfilled. 

\section{Outlook}
In the previous chapter we mentioned several ways to improve the results of this 
thesis. Here we give a brief summary.

The simplest way to improve the performance of a machine learning model is to give 
it more, high quality training data. To our knowledge there now exists three 
additional datasets we have not used in this dataset \citep{CAMELS_CL, CAMELS_AU, CAMELS_BR}.

An LSTM based model with a one-dimensional CNN input layer, intended to reduce 
the number of time steps needed, thus reducing the computational cost of training 
the LSTM.

Improve the hyperparameter tuning of our models. This could be especially important 
for transfer learning, as the reduction of variance is often important for 
generalization \citep{elemstatlearn}. This could be a more manageable task if 
the aforementioned CNN layer is implemented to decrease training time.

Introduce a physics based penalty term to the cost function. A simple example of 
this is described in the previous chapter. This could improve the interpretability 
of an LSTM and also increase the consistency of the output, as put by \citet{hybrid_paper}.

Use a hybrid model approach. This could lead to better generalizability and 
interpretability. There are several ways to do this, the most thorough being to 
implement an LSTM instead of a given process in a process-driven model. This could 
then be used to compare to the output of said process in the ordinary model versus 
the hybrid model. This is likely very computationally expensive, as the process-driven 
models are more expensive than pure LSTM models. A simpler hybrid model discussed 
in this thesis is a setup where the LSTM is trained to correct the error of 
the output of a traditional model. This way one could leverage the already existing 
model benchmarks on CAMELS \citep{CAMELS_hydroshare}.

One should also look into the possibility of implementing LSTM models in Statkraft's 
Shyft\citep{Shyft}, making it much easier to employ these models on many datasets, 
as well as greatly simplifying the process of comparing with other models.
