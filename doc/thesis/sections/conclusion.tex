\section{Future Work}
There are several ways to take the work done in this thesis further. Some of the 
main points are summarized in the following list:
\begin{itemize}
\item As mentioned several times in this thesis the only way to show that a machine learning model performs well in a specific use case is to do extensive empiric research. This means that a similar analysis needs to be performed on similar data from different sources. Going further with this would then mean to find or create a new dataset with different datasources than \cite{CAMELS_GB} or \cite{CAMELS_US}. During the initial research phase of this thesis we discovered that there was an effort in Chile to create a similar dataset \cite{CAMELS_CL}. The dataset did not seem to be of the same quality as what was used in this thesis so we decided it was out of scope to try and manage that as well.
\item The scope of this thesis work was limited to LSTM models only. LSTM models have as mentioned in chapter \ref{LSTM drawbacks} some drawbacks that could lead one into trying a different machine learning model. Lately Convolutional Neural Networks (CNNs), which usually are used for image analysis, have shown promising results on time series data as well. \citationneeded CNNs are much more parallelizable algorithms than RNNs \citationneeded and could prove to be more efficient at doing training and prediciton. Different RNNs could also be considered.
\item An often overlooked but in fact very important aspect of machine learning models is the tuning of hyperparameters. As mentioned in \ref{Method Drawbacks or something} the approach in this thesis is perhaps not thorough enough. To the author's best knowledge the best way to tune hyperpameters is in fact using a method called nested cross validation \citationneeded. This would take too long to train to be a viable approach for our work, but should be looked into in the future.
\end{itemize}
