In this chapter we discuss the performance of our LSTM models. We discuss the implications 
our results and the results of similar works may have on the physical understanding 
of rainfall-runoff modelling. In addition we discuss potential ways to improve machine 
learning models for rainfall-runoff modelling, with focus on performance and 
interpretability.
\section{Model Selection}
\label{discuss model selection}
The results presented in Section \ref{CAMELS-GB results}-\ref{Mixed results} 
all agree that performance is increased when introducing static attributes. 
In addition to this, we see that the ordinary LSTM models that treat the static 
attributes as time series are the highest performing models. We find it likely that 
this is due  to the decrease in model complexity in an EA-LSTM compared to an LSTM. 
For example, Mixed$_\text{ea-lstm}$ has 205057 trainable parameters (weights and 
biases), Mixed$_\text{lstm}$ has 285953 and Mixed$_\text{none}$ has 266497. The 
EA-LSTM is a less complex model than an ordinary LSTM trained without 
static attributes. The EA-LSTM's performance coming close to that of an 
LSTM, while only using $~72\%$ of the trainable parameters is notable, however. 
It could imply 
that even a less complex model benefits from the additional information 
contained in the attributes. 
In all cases but the case of transfer learning, our best performing models are the 
most complex model with the highest amount of static attributes possible. For 
prediction on CAMELS-GB that means GB$_\text{lstm, all}$, for CAMELS the best model 
is US$_\text{Kratzert}$, for the datasets mixed the best model is Mixed$_\text{lstm}$, 
and for transfer learning: Transfer$_\text{US, none}$.

We do not spend significant time in this thesis tuning hyperparameters. Instead, 
we choose to use the hyperparameters derived by \citet{lstm_second_paper}. The one 
exception to this is for models trained on CAMELS-GB, shown in Section 
\ref{CAMELS-GB results}. We found 
that using dropout in this case at best makes no difference and at worst slightly 
decreases performance. In some cases we also tested whether increasing the 
sequence length from 270 days to 365 days increases performance, but our results 
indicate that this leads to no performance difference. To save memory and decrease 
training time we therefore keep the lower sequence length of 270 days.

We acknowledge that it may be possible to further increase the performance by doing 
a thorough hyperparameter tuning experiment. The tuning done by \citet{lstm_second_paper} 
is a simple grid search with few points, and is not guaranteed to be a good basis, 
especially since this tuning was done for gauged basin prediction. Hyperparameters 
are likely to have different optimal values for our case of ungauged prediction. A very thorough 
hyperparameter tuning would be hard to brute force, due to the computationally 
expensive nature of our models, therefore one would have to implement a more sophisticated 
method than mere exhaustive grid searching. This becomes even more computationally 
expensive if one wants to tune using cross validation.

We find no clear signs of overfitting in our selected models when comparing 
cross validated performance to test performance. Table 
\ref{results summary table} indicates 
similar performance between the cross validated models and the refit models 
validated on the test set. There are some discrepancies in the CDF plot of 
the performance of the models trained on CAMELS 
shown in Figure \ref{test cdf} (second from top), leading to a median NSE of 
0.65 on the test set as opposed to $0.68$ in the cross validated case. This small 
deviation could perhaps be explained by the findings of \citet{lstm_second_paper},
 which found that an ensemble of LSTM models performs better than a singular LSTM 
 because of nontrivial differences in performance caused by randomness in the 
 training process. For computational reasons we have chosen not to use model 
 ensembles anywhere in this thesis.

\section{Performance and Importance Analysis}
\label{discuss static attributes}
\subsection{CAMELS-GB}
Table \ref{overfit table} shows among others \textbf{\texttt{Q95}} as the most 
important attribute for the overfit "proof of concept" model. This attribute is the 
95th percentile runoff value for the full runoff time series for a given basin, 
which is information directly derived from the expected outcome. 
Figure \ref{CAMELS-GB CDF validation} also shows that the overfit model drastically 
outperforms all other models.
Because of this, we believe it is 
safe to assume that an LSTM model can learn which static attributes are 
the most important in CAMELS-GB, at least in the most obvious case. In addition, 
it also indicated that the permutation feature importance algorithm succeeds at deciding the most important 
static attribute in this case. These assumptions are therefore used to strengthen 
our confidence in the results from our other, non-overfit models. 

Looking again at Figure \ref{CAMELS-GB CDF validation} along with Table 
\ref{results summary table}, we see that the median NSE value of GB$_\text{lstm, all}$ 
is approximately $0.82$ cross validated, with a test score of $0.80$. 
This performance is vastly superior to that found by \citet{lstm_third_paper}, and 
is even better than gauged prediction on CAMELS, such as \citet{lstm_second_paper}. 
This makes sense when one considers the relatively low variance in climate in Great 
Britain, compared to the United States. CAMELS-GB consists of the same amount 
of basins as CAMELS, but with less variance this means the basins in the validation 
sets will be more similar to the ones in the training set, making it less critical 
for the model to generalize.
As far as the author knows, there are no published benchmarks using traditional models on 
CAMELS-GB. Creating such a benchmark in the future is necessary for us to properly 
assess model performance.

In Figure \ref{best prediction all concat seq len}, we see that at least for the 
best performing prediction, the high NSE score does 
in fact transfer to a visually impressive performance. Despite some peaks not being 
correctly simulated, the LSTM model manages to replicate the observed runoff in a 
very acceptable manner.

As the results presented in Table \ref{all concat table}
indicate that the many of the important static attributes in CAMELS-GB (as perceived by 
GB$_\text{lstm, all}$) are derived directly from the 
input time series (the most important attribute being \textbf{\texttt{low\_prec\_dur}}, for instance), 
it is likely that we cannot extract much new information from 
these, as this information should be available in the time series already used by 
the models.
If this were the case for CAMELS-GB only, one could perhaps assume it being caused 
by CAMELS-GB having more similar basins, but later we see that this is also the 
case for models predicting on CAMELS.
The reason for this being the case is likely connected to the 
nature of the LSTM training process and the sequence length. Ways 
to further improve the long-term memory of an LSTM should be further explored 
in the future. The biggest examples of this type of attribute are the top two ranked 
attributes: \textbf{\texttt{low\_prec\_freq}} and \textbf{\texttt{low\_prec\_dur}}. 
These severely outrank all other static attributes, especially for the 75th percentile 
of importances, and they are directly derived from the precipitation time series.
They are defined as the frequency of days with $\leq 1$ mm$/$day precipitations, and 
the average duration of these dry periods respectively.
Other than derived features, the model ranks the other attributes relatively similarly 
for the upper 75th percentile. These other attributes mostly contain information 
about land cover and elevation levels, which is information an LSTM cannot access 
through the time series contained in CAMELS-GB. Process-driven models such as VIC 
use information such as land coverage and elevation by splitting the catchment 
into a spatial grid, it is therefore unlikely that any of these attributes can 
contribute to any major improvements for process-driven models for predicting 
on CAMELS-GB. To summarize: Our model tends to mostly prefer static attributes 
containing information already available in the time series, it is therefore hard 
to argue that the static attribute ranking found here can be used to improve 
traditional models, which already use this information via the time series.

\subsection{CAMELS}
Looking at Figure \ref{CAMELS-US CDF validation} and Table \ref{results summary table}, 
the performance of US$_\text{Kratzert}$ coincides with the results of \citet{lstm_third_paper}, 
which is what we wanted to replicate. The performance is generally lower than that 
of GB$_\text{lstm, all}$, which makes sense when one considers how much more diverse 
the United States climate is, compared to the climate of Great Britain. 

The best prediction made by US$_\text{Kratzert}$ is very accurate and has an NSE of $0.92$. 
This is lower than for the best prediction on CAMELS-GB. Similarly to the previous 
section, we here too
see that the prediction fails to predict the higher peaks of runoff. 
This is also something observed by \citet{lstm_first_paper}.

Table \ref{kratzert importance} shows that the permutation algorithm yields different 
results than the feature ranking done by \citet{lstm_second_paper}. Said paper 
averaged the normalized sensitivity (derived by what they call the explicit Morris 
method) of each basin, while we present percentiles and rank based on the median
\footnote{Also note that we predict on ungauged basins, in contrast to said paper. 
This could differentiate important attributes.}. 
Looking at our median scores, we see that the majority of basins do not benefit 
from the inclusion of static attributes. Looking at the upper percentiles, the 
most important attributes in our case still somewhat agree with those of 
\citet{lstm_second_paper}. Topological information and climatic indices are important. 
By far the most important attributes for the 75th percentile are aridity and 
catchment area. Our results do however favour land coverage types and soil types 
more than the results of \citet{lstm_second_paper}, and the attribute deemed the most important by 
said paper, \textbf{\texttt{p\_mean}}, is only significantly important 
for the 95th percentile of basins. As expected, \textbf{\texttt{frac\_snow}} is 
much more important for CAMELS than for CAMELS-GB, as there is more snow 
in certain regions in the United States than in Great Britain. This is also reflected in the fact that 
\textbf{\texttt{frac\_snow}} is important for a small portion of British basins (the 95th percentile), 
while it is more important both for the 75th and 95th percentile in the US. 

\subsection{Mixed model}
Our best model trained on both CAMELS and CAMELS-GB seems to find static attributes
more important 
than the models trained on the datasets individually. Table \ref{importance mixed} 
shows that the model also ranks the attributes differently. Figure \ref{mixed CDF validation} 
shows that the addition of static attributes is of significantly higher benefit for 
prediction on CAMELS-GB than for CAMELS although the performance per dataset is not 
very far off the individually trained models (see Figure \ref{results summary table}).
This could be consistent with the importance rankings, as 
the median importances, and especially the 75th percentile and above importances, are 
significantly higher on CAMELS-GB than those on CAMELS. There is a chance that this 
is due to the model learning to use the attributes to differentiate between the 
two datasets, without actually extracting important information.
This could also be because CAMELS-GB is more homogeneous, and shuffling the 
attributes in CAMELS-GB therefore has a lesser effect than shuffling those of 
CAMELS.

An interesting detail is that the permutation algorithm yields negative importance 
for some basins. This happens for all models mentioned in this section. 
This implies that our LSTM models actually perform worse 
on some basins because of the inclusion of static attributes, perhaps due to a 
lack of training data representing similar basins in the dataset. This is likely 
alleviated by introducing more data. We believe other ways to rank attributes 
should also be attempted 
in the future. \citet{lstm_second_paper} used a robustness test which 
involves gradually adding Gaussian noise to a feature and evaluating how it 
affects model performance. \citet{OrigCAMELSRanking} 
uses a different method altogether, the results of which \cite{lstm_second_paper} 
agree with. To our knowledge there exists no feature ranking results on CAMELS-GB 
apart from what is shown in this thesis, 
so these two other methods should also be attempted on CAMELS-GB in addition 
to the permutation feature importance algorithm employed in this thesis to get further context and comparability.

\subsection{Transfer learning}
The results in Section \ref{Transfer learning section} show that the models trained 
for transfer learning do not benefit from adding static attributes. Therefore, we 
do not include a permutation algorithm for this case, We also see 
that the case of US$\rightarrow$GB significantly improves when removing attributes. This makes 
sense as Great Britain has much smaller climatic variance than the United States 
(see for instance Figure \ref{attribute comparison}). We believe these two phenomena could be related. It is likely 
that the relatively lower performance from using static attributes comes from the 
fact that the datasets have different climatic conditions. There could also be 
room for improvement in how we preprocess both the attributes and the time series. 
As seen in Table \ref{attribute transfer} we assume that the temperature time series 
of CAMELS behave like symmetrical periodic functions, therefore leading to 
\begin{equation}
T_\text{average} \approx  \frac{T_\text{min}+T_\text{max}}{2}. \label{dumb assumption}
\end{equation}
Ideally, we need to have temperature as a time series for a given model to be able 
to properly model snow and frozen soil accumulation, which is very important for 
hydrologic models. The fact that models trained on both datasets 
still perform comparably to models trained on only one dataset seems to suggest 
that (\ref{dumb assumption}) is not entirely destructive, however.
In the later stages of this thesis it was discovered that the average temperature 
is actually present in CAMELS. The older forcing data \citep{maurer} contains 
this information stored as duplicates in the maximum and minimum temperature time 
series. Due to time constraints, we are unable to train models utilizing this. 
This is likely a good start for those looking to improve transfer learning 
performance. 
The best outcome is likely to come from finding maximum and minimum temperatures 
for CAMELS-GB, however.

It seems to us that there are several angles to further tackle the issue of transfer 
learning between these datasets. One of which we stated above: Find maximum and minimum time series for 
CAMELS-GB. A second way is to improve the preprocessing of static attributes to 
the point where one would hopefully see better performance with attributes than 
without.  

While we stated earlier in this chapter that we see no clear signs of overfitting, this is not 
the case for transfer learning. Figure 
\ref{training progress transfer} indicates that Transfer$_\text{GB, none}$ 
and Transfer$_\text{US, none}$ start overfitting on their respective training 
datasets at epoch one and five respectively. This overfitting is not apparent when
validating on basins from the 
same dataset as they are trained on, but very quickly manifests when validating 
on the other dataset. While improved pre-processing, regularization and the gathering 
of more  cross-compatible data could alleviate this problem, it is likely that this 
overfitting is 
unavoidable using the current datasets. More data, both in quantity and in 
diversity, is likely needed for the model's training process to be able to 
successfully predict on a dataset not included during training.

There now exist several other similar datasets to the two used for this study. 
The easiest way to improve 
any machine learning model is often to provide more data for training. A Chilean 
dataset consisting of 515 catchments by the name CAMELS-CL \citep{CAMELS_CL} 
was studied early in this thesis work, but the quality 
of the data was deemed worse than that of CAMELS and CAMELS-GB. With some initial 
preprocessing it could be of interest to see whether it is feasible to use this 
dataset.
In addition we know of two other datasets. One goes by the name CAMELS-BR, and is 
a Brazilian dataset consisting of 897 catchments \citep{CAMELS_BR}.
The other is CAMELS-AU, and is an Australian dataset consisting of 222 catchments
 \citep{CAMELS_AU}. Especially CAMELS-BR seems to be of interest due to the 
amount of catchments. Combining datasets is a non-trivial, time consuming task, which 
we expect will become increasingly hard the more datasets one tries to combine. 
It is not unlikely that one has to discard almost all static attributes to make 
the datasets compatible. We therefore expect that some effort to gather new data 
would also be necessary. It is of course also of interest to see how well machine 
learning models perform on one or more of these datasets in isolation. 
We see this as a possible candidate for the start of a new master thesis.
\section{Comparison to traditional models}
We are able to reproduce the results of \citet{lstm_second_paper,lstm_third_paper}. 
Our models 
trained on CAMELS perform significantly better than traditional models. In this 
thesis we compare to two process-driven models, VIC and NWM, and one conceptual 
model, SAC-SMA. 

What we additionally show in this thesis is that models trained on both CAMELS and CAMELS-GB 
also perform better on CAMELS than the addressed traditional models. This could 
be because the models are complex enough to recognize which dataset a given 
basin is from based on underlying signatures in either the time series or the 
static attributes. Perhaps this differentiation could stem from differences in 
data collection practice. 

One of many reasons why LSTM models perform better than traditional models on CAMELS in general 
could be that the LSTM 
models are better at modelling basins with special conditions leading to 
a rainfall-runoff ratio different from 1. When the rainfall-runoff ratio is lower 
than one it usually implies that the rainfall is either stored as snow or frozen 
soil, that it is vaporized through photosynthesis in local vegetation, or that there 
is some difficult to model complexity to the subsurface flow in the area. 
These processes are difficult to model compared to runoff directly caused by 
rainfall \citep{process-driven-history}.
Our results may indicate that this could be the case.
Figure \ref{runoff ratio} shows that only the traditional models have 
a statistically significant linear relationship between performance and 
rainfall-runoff ratio. The relationships are generally weak, with a Pearson 
correlation between 0.24 and 0.33, but they are more apparent than 
for the LSTM models, where there exists no such linear relationship. The impact 
of this discovery is quite uncertain, however, as there is significant 
Spearman correlation (nonlinear correlation) for all the models, ranging 
from 0.31 to 0.53.

The LSTM models are, as briefly mentioned earlier, much less complex spatially. 
While process-driven models need to grid the 
spatial dimension to properly model the physical system, an LSTM model finds 
relationships between input and output data in a one-dimensional manner. This could 
make LSTMs and other machine learning methods valuable models to use when there is not 
enough data available to properly set up a traditional model such as the NWM.
