
% REMEMBER TO SET LANGUAGE!
\documentclass[a4paper,12pt,english]{article}
\usepackage{gensymb}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, vmargin=0.75in, hmargin=0.7in]{geometry}

% Standard stuff
\usepackage{amsmath,graphicx,varioref,verbatim,amsfonts}
% colors in text
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
% Hyper refs
\usepackage[colorlinks]{hyperref}
\usepackage{float}
\usepackage{cite}
% Document formatting
\setlength{\parindent}{0mm}
\setlength{\parskip}{1.5mm}
\usepackage{tikz}
%Color scheme for listings
\usepackage{textcomp}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}

%Listings configuration
\usepackage{listings}
\usepackage{subcaption}
\usepackage{physics}
%Hvis du bruker noe annet enn python, endre det her for å få riktig highlighting.
\lstset{
	backgroundcolor=\color{lbcolor},
	tabsize=4,
	rulecolor=,
	language=python,
        basicstyle=\scriptsize,
        upquote=true,
        aboveskip={1.5\baselineskip},
        columns=fixed,
	numbers=left,
        showstringspaces=false,
        extendedchars=true,
        breaklines=true,
        prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
        frame=single,
        showtabs=false,
        showspaces=false,
        showstringspaces=false,
        identifierstyle=\ttfamily,
        keywordstyle=\color[rgb]{0,0,1},
        commentstyle=\color[rgb]{0.133,0.545,0.133},
        stringstyle=\color[rgb]{0.627,0.126,0.941}
        }




%opening
\title{LSTM research}
\author{Bernhard Nornes Lotsberg}

\begin{document}
\maketitle
\section*{LSTM}
\begin{itemize}
\item RNN with long and short term memory.
\item Long term memory not possible in ordinary RNN's because of vanishing gradients.
\item Long term memory is very important for runoff modelling, as some features take a long time to impact. Snow for example.
\end{itemize}

\section*{EA-LSTM}
\begin{itemize}
\item Paper \cite{lstm_second_paper} introduces the EA-LSTM (Entity Aware), which is a modified version of the LSTM designed to better with this this problem.
\item Beneficial for hydrological modelling because it is able to process information about the current catchment that it is modelling (?). 
\end{itemize}

\section*{Usage of existing code}
\begin{itemize}
\item Notebook tutorial for using LSTM used in paper \cite{lstm_first_paper} can be found at \url{https://github.com/kratzert/pangeo_lstm_example}
\begin{itemize}
\item This repository includes a binder link for running the notebook in browser.
\item Notebook shows easy to understand examples of how to use Pytorch and how to load the CAMELS dataset.
\item Only the first experiment in the paper.
\end{itemize}
\item Code for paper \cite{lstm_second_paper} can be found at \url{https://github.com/kratzert/ealstm_regional_modeling}
\begin{itemize}
\item This github page contains links to datasets used as well as pre trained models. 
\item Need stronger computer to recreate the models, but should hopefully be possible to use the pre trained models from laptop.
\item Runs fine on my home desktop.
\item All results in the paper and all relevant code is here, including pre-trained models and code for creating and training models yourself.
\end{itemize}
\item Code for paper \cite{lstm_third_paper} can be found at \url{https://github.com/kratzert/lstm_for_pub}
\begin{itemize}
\item Includes step-by-step guide on how to recreate results from article. (Including bash scripts for automation)
\item Need Matlab for a few plots, but unsure if this is important
\end{itemize}
\item Spend most time on the first paper in the beginning, it is important to actually understand how an LSTM model works (not to mention RNNs in general!).
\item Paper \cite{lstm_third_paper} seems to take a lot more computational resources to recreate. It performs tuning of hyperparameters as well as using an ensemble of ten independently trained LSTMs.
\end{itemize}

\section*{Purely data driven model}
\begin{itemize}
\item A lot of code already exists
\item First thing to try is to explicitly provide snow data (which is not done by papers cited in this document).
\item Possibly unreliable as we have less physical intuition, but unsure if a hybrid model performs better.
\item LSTM or EA-LSTM
\item There is room for improvement in a basic manner as well, as the papers state that a proper hyperparameter search hasn't been performed. This is probably time consuming and boring, though. (Kratzert has done this better in later research according to the talk)
\item Also, only simple single and double layer LSTMs have been used, with few hidden cells. Parameters in the lower hundreds.
\item Humidity should be considered, as it is not provided to the model right now according to the talk given my Kratzert.
\item Integrated gradients can be used to interpret model. \url{https://github.com/hiranumn/IntegratedGradients}
\end{itemize}

\section*{Hybrid model}
\begin{itemize}
\item Could be more intuitive, possibly also more reliable.
\item Trained using Shyft in the training process. Using the output from Shyft as an input for instance.
\item Probably more difficult to train.
\item LSTM or EA-LSTM
\end{itemize}

\section*{Choice of model}
\begin{itemize}
\item I think starting with a purely data driven model and then try introducing output from Shyft along with data not used by Shyft.
\item Several configurations need to be tested.
\item Before any of this I need to actually learn how to use Pytorch and how LSTMs work.
\item Paper \cite{lstm_third_paper} suggests that physical constraints applied to the LSTM models could improve results.
\item Hybrid model should at least be attempted!
\item All papers referenced use the CAMELS dataset, could be interesting to try data from Statkraft.
\end{itemize}

\section*{Meaning of "physical constraints" in paper \cite{lstm_third_paper}}
\begin{itemize}
\item The SAC-SMA model performs better than the LSTM models in some cases, likely because some physical phenomena aren't learned by the LSTM. 
\item Not much to go by here really, need to find out where to start from if this is the way I want to go forward.
\item A main point of paper \cite{lstm_third_paper} is that there is still a lot of data not used by the traditional physical models that the LSTM models benefit from, while there are some cases where the physical models outperform the LSTM because of overfitting to catchments where this isn't the case.
\item "Most effective strategy" will be theory-guided data science, main conclusion of article.
\item Many sciences suffer from the curse of dimensionality.
\item Scientific discoveries aren't just ways to make predictions, we actually wish to understand underlying processes. This is very important in fields where wrong predictions are dangerous or costly (as with our case).
\end{itemize}

\section*{Theory-guided data science (TGDS)}
\begin{itemize}
\item Described in paper \cite{hybrid_paper}
\item The paper proposes strategies for using existing scientific knowledge along with machine learning. Likely to be a very important source for this thesis.
\item A way to limit model space without increasing model bias as much as ordinary regularization. In other words: Reducing variance while not affecting bias using existing knowledge of a scientific problem.
\item Performance $\propto$ Accuracy $+$ Simplicity $+$ \textit{Consistency}
\item Consistency here refers to consistency with existing knowledge.
\item Paper mentions five TGDS strategies:
\begin{enumerate}
\item Using scientific knowledge to design models specifically for a given scientific problem or by designing a desirable custom loss function. (Se the usage of NSE in Paper \cite{lstm_first_paper} - \cite{lstm_third_paper}). The EA-LSTM could perhaps be seen as an example of a specifically designed model. Another example is CNNs for image analysis.
\item Guiding a model by for example initializing it with meaningful parameters or implementing a specialized form of regularization.
\item Refining the output of a model using scientific knowledge.
\item Hybrid model where a physical model works in tandem with a machine learning model. 
\item data science methods can also help in augmenting theory-based models to make effective use of observational data. (Don't understand what this means, just copy/pasted it.)
\end{enumerate}
\item Point 3 and 4 here sound somewhat similar to what we discussed in brainstorm.
\end{itemize}

\section*{Paper \cite{lake}}
\begin{itemize}
\item Modern pure physical models are prone to overfitting and often employ exhaustive search for parameters.
\item Based on paper \cite{hybrid_paper}.
\item Introduces new RNN called Physics-Guided Recurrent Neural Network (PGRNN)
\item Need to read more
\end{itemize}

\section*{Plan forward}
\begin{enumerate}
\item Brainstorm with Miro and/or Simon. Discuss the idea of training the model on a simple dataset and how this would work in practice. This would also give me better insight in how to use Pytorch and LSTM models. Paper \cite{hybrid_paper} suggests that training on a simple but relevant dataset before training on a more specific and complex case has been shown to be beneficial.
\end{enumerate}

\bibliographystyle{IEEEtran}
\bibliography{references_notes_lstm}
\end{document}

